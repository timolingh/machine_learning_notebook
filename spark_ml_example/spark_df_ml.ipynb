{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning example with Spark Dataframe and ML (not MLib)\n",
    "This is a modification of an example I found online [here](http://www.techpoweredmath.com/spark-dataframes-mllib-tutorial/).  The example used the MLib library for machine learning applications. I wanted an example that relied primarily on the Dataframe object and not RDD - supposedly better performance - and the ML library instead of the MLib. It is my understanding that ML is recommended for dataframes.\n",
    "\n",
    "This notebook has 2 sections: Importing and munging data and fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Packages to use\n",
    "# Note in batch mode, a Spark context and Sql Context will need to be imported and setup (see below)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interacvtive (pyspark) mode loads the spark context automatically.  This is needed for batch mode\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession\\\n",
    "#     .builder\\\n",
    "#     .appName(\"test1.py\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# As an example\n",
    "# Possible to read in as dataframe?? \n",
    "# df = sc.textFile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and preparing the data\n",
    "I did minimal productive work here other than importing the data.  I did try some basic column operations here, but eventually I concluded that you should try to do as much of that type of work outside of Spark.  Spark has a lot of the functionality that you will find in an RDBMS, but I find it difficult to use and inconsistent in the results. My recommendation: do as much as you can in SQL before you get it into Spark for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Users/tim/pyspark/data/Sacramentorealestatetransactions.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- baths: integer (nullable = true)\n",
      " |-- sq__ft: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- sale_date: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+-----+----+------+\n",
      "|          street|           sale_date|price|baths|beds|sq__ft|\n",
      "+----------------+--------------------+-----+-----+----+------+\n",
      "|    3526 HIGH ST|Wed May 21 00:00:...|59222|    1|   2|   836|\n",
      "|     51 OMAHA CT|Wed May 21 00:00:...|68212|    1|   3|  1167|\n",
      "|  2796 BRANCH ST|Wed May 21 00:00:...|68880|    1|   2|   796|\n",
      "|2805 JANETTE WAY|Wed May 21 00:00:...|69307|    1|   2|   852|\n",
      "| 6001 MCMAHON DR|Wed May 21 00:00:...|81900|    1|   2|   797|\n",
      "+----------------+--------------------+-----+-----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select specific fields\n",
    "df_model = df.select('street', 'sale_date', 'price', 'baths', 'beds', 'sq__ft')\n",
    "df_model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(street=u'3526 HIGH ST', sale_date=u'Wed May 21 00:00:00 EDT 2008', price=59222, baths=1, beds=2, sq__ft=836),\n",
       " Row(street=u'51 OMAHA CT', sale_date=u'Wed May 21 00:00:00 EDT 2008', price=68212, baths=1, beds=3, sq__ft=1167),\n",
       " Row(street=u'2796 BRANCH ST', sale_date=u'Wed May 21 00:00:00 EDT 2008', price=68880, baths=1, beds=2, sq__ft=796),\n",
       " Row(street=u'2805 JANETTE WAY', sale_date=u'Wed May 21 00:00:00 EDT 2008', price=69307, baths=1, beds=2, sq__ft=852),\n",
       " Row(street=u'6001 MCMAHON DR', sale_date=u'Wed May 21 00:00:00 EDT 2008', price=81900, baths=1, beds=2, sq__ft=797)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The less pretty output\n",
    "df_model.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells just have a few examples of my trying to manipulate the columns in Spark. This is what eventually convinced me to avoid doing this type of stuff in Spark - so much easier in SQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just for kicks, I'd like to add a derived variable.  This is a map operation to create a new column\n",
    "# This works in some older examples, but does not in 2.1.0 - so annoying\n",
    "\n",
    "# foo = df_model.select('price').map(lambda x: x * 1000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the above did not work - let's try again\n",
    "# this is kind of stupid.  convert to rdd to perform a map.\n",
    "# but then you have to convert back to DF   STOOPID!!\n",
    "\n",
    "# foo = df_model.select('price').rdd.map(lambda x: (x[0] * 100))\n",
    "# foo.collect()[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: foo is not a list\n",
    "# type(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# is there a native dataframe method that will do it?\n",
    "# this is the way to do it!\n",
    "# but it appears to be not complete\n",
    "# Need to import stuff for functions\n",
    "\n",
    "# df_model = df_model.withColumn('price_new', (df_model.price * 1000))\n",
    "# df_model.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with ML\n",
    "My primary goal here is to simply get the table into the correct format for the ML libraries.  I did not do all the proper things you are supposed to do like set up training and validation splits. Nor did I do all of the regression diagnostics.  I'll probably save that for another day.\n",
    "\n",
    "For someone who is used to fitting models in R or even regular Python, this was an amazingly frustrating experience.  I hope this example spares you some of that frustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note.  The cell right below this is the MLib way to start building the input dataset.  Most of the examples I found online used this approach.  I'm leaving it here for reference, but it is not what I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to label.  why is this not a built-in function???\n",
    "# building this from all the mlib examples\n",
    "# this seems to be the way for RDDs but doesn't work for spark dataframes\n",
    "\n",
    "# def labelData(data):\n",
    "#     return data.map(lambda row: LabeledPoint(row[-1], row[:-1]))\n",
    "\n",
    "# df_design = labelData(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street', 'sale_date', 'price', 'baths', 'beds', 'sq__ft']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|baths|beds|sq__ft|\n",
      "+-----+----+------+\n",
      "|    1|   2|   836|\n",
      "|    1|   3|  1167|\n",
      "|    1|   2|   796|\n",
      "|    1|   2|   852|\n",
      "|    1|   2|   797|\n",
      "+-----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the predictoes for the regression model\n",
    "df_x = df_model.drop('price', 'price_new', 'street', 'sale_date')\n",
    "df_x.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+-----+----+------+----------------+\n",
      "|          street|           sale_date|label|baths|beds|sq__ft|        features|\n",
      "+----------------+--------------------+-----+-----+----+------+----------------+\n",
      "|    3526 HIGH ST|Wed May 21 00:00:...|59222|    1|   2|   836| [1.0,2.0,836.0]|\n",
      "|     51 OMAHA CT|Wed May 21 00:00:...|68212|    1|   3|  1167|[1.0,3.0,1167.0]|\n",
      "|  2796 BRANCH ST|Wed May 21 00:00:...|68880|    1|   2|   796| [1.0,2.0,796.0]|\n",
      "|2805 JANETTE WAY|Wed May 21 00:00:...|69307|    1|   2|   852| [1.0,2.0,852.0]|\n",
      "| 6001 MCMAHON DR|Wed May 21 00:00:...|81900|    1|   2|   797| [1.0,2.0,797.0]|\n",
      "+----------------+--------------------+-----+-----+----+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the price column to 'label'\n",
    "df_model = df_model.withColumnRenamed('price', 'label')\n",
    "\n",
    "# Create feature vector - a vector comprising the values of the three predictors\n",
    "pred_col = df_x.columns\n",
    "assembler = VectorAssembler(inputCols = pred_col, outputCol = \"features\")\n",
    "df_model_fit = assembler.transform(df_model)\n",
    "\n",
    "df_model_fit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_model_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street', 'sale_date', 'label', 'baths', 'beds', 'sq__ft', 'features']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The original columns are still there\n",
    "df_model_fit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the linear regression object and \n",
    "# Use the fit method. The result is a model\n",
    "# object\n",
    "lr = LinearRegression()\n",
    "model = lr.fit(df_model_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.009326287108706e-13, 0.17614914553420657, 0.0035325570725432964, 0.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p-values\n",
    "model.summary.pValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+-----+----+------+----------------+------------------+\n",
      "|              street|           sale_date|   label|baths|beds|sq__ft|        features|        prediction|\n",
      "+--------------------+--------------------+--------+-----+----+------+----------------+------------------+\n",
      "|        3526 HIGH ST|Wed May 21 00:00:...| 59222.0|    1|   2|   836| [1.0,2.0,836.0]|184127.19166461288|\n",
      "|         51 OMAHA CT|Wed May 21 00:00:...| 68212.0|    1|   3|  1167|[1.0,3.0,1167.0]|182508.27125869133|\n",
      "|      2796 BRANCH ST|Wed May 21 00:00:...| 68880.0|    1|   2|   796| [1.0,2.0,796.0]|183354.65708525322|\n",
      "|    2805 JANETTE WAY|Wed May 21 00:00:...| 69307.0|    1|   2|   852| [1.0,2.0,852.0]|184436.20549635676|\n",
      "|     6001 MCMAHON DR|Wed May 21 00:00:...| 81900.0|    1|   2|   797| [1.0,2.0,797.0]|183373.97044973721|\n",
      "|  5828 PEPPERMILL CT|Wed May 21 00:00:...| 89921.0|    1|   3|  1122|[1.0,3.0,1122.0]| 181639.1698569117|\n",
      "| 6048 OGDEN NASH WAY|Wed May 21 00:00:...| 90895.0|    2|   3|  1104|[2.0,3.0,1104.0]|243187.63543899695|\n",
      "|       2561 19TH AVE|Wed May 21 00:00:...| 91002.0|    1|   3|  1177|[1.0,3.0,1177.0]|182701.40490353125|\n",
      "|11150 TRINITY RIV...|Wed May 21 00:00:...| 94905.0|    2|   2|   941| [2.0,2.0,941.0]|248051.20107822906|\n",
      "|        7325 10TH ST|Wed May 21 00:00:...| 98937.0|    2|   3|  1146|[2.0,3.0,1146.0]|243998.79674732458|\n",
      "|    645 MORRISON AVE|Wed May 21 00:00:...|100309.0|    2|   3|   909| [2.0,3.0,909.0]| 239421.5293646186|\n",
      "|       4085 FAWN CIR|Wed May 21 00:00:...|106250.0|    2|   3|  1289|[2.0,3.0,1289.0]| 246760.6078685354|\n",
      "|     2930 LA ROSA RD|Wed May 21 00:00:...|106852.0|    1|   1|   871| [1.0,1.0,871.0]|192814.80347167535|\n",
      "|       2113 KIRK WAY|Wed May 21 00:00:...|107502.0|    1|   3|  1020|[1.0,3.0,1020.0]|179669.20667954456|\n",
      "| 4533 LOCH HAVEN WAY|Wed May 21 00:00:...|108750.0|    2|   2|  1022|[2.0,2.0,1022.0]| 249615.5836014324|\n",
      "|      7340 HAMDEN PL|Wed May 21 00:00:...|110700.0|    2|   2|  1134|[2.0,2.0,1134.0]|251778.68042363945|\n",
      "|         6715 6TH ST|Wed May 21 00:00:...|113263.0|    1|   2|   844| [1.0,2.0,844.0]| 184281.6985804848|\n",
      "|6236 LONGFORD DR ...|Wed May 21 00:00:...|116250.0|    1|   2|   795| [1.0,2.0,795.0]|183335.34372076922|\n",
      "|     250 PERALTA AVE|Wed May 21 00:00:...|120000.0|    1|   2|   588| [1.0,2.0,588.0]|179337.47727258297|\n",
      "|     113 LEEWILL AVE|Wed May 21 00:00:...|121630.0|    2|   3|  1356|[2.0,3.0,1356.0]| 248054.6032889628|\n",
      "+--------------------+--------------------+--------+-----+----+------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicted values appended to the original table\n",
    "model.summary.predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
